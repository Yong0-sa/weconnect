"""
Utilities for running a RAG (Retrieval-Augmented Generation) workflow.

This module exposes a RAGService class that encapsulates:
* moderation checks
* greeting / fallback prompt construction
* embedding + retrieval against ChromaDB
* GPT response generation

It is designed to be imported by the FastAPI layer, so no CLI interaction
remains in this file.
"""

from __future__ import annotations

import os
import re
from dataclasses import dataclass
from typing import List, Literal, Optional, Sequence

from chromadb import PersistentClient
try:
    from chromadb.errors import InvalidCollectionException, NotFoundError
except ImportError:  # chromadb>=0.5 drops NotFoundError
    from chromadb.errors import InvalidCollectionException

    class NotFoundError(InvalidCollectionException):  # type: ignore[misc]
        """Fallback for older chromadb API expectation."""
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


class RAGServiceError(RuntimeError):
    """Base class for RAG related errors."""


class EmptyQueryError(RAGServiceError):
    """Raised when the user query is empty."""


class InappropriateQueryError(RAGServiceError):
    """Raised when OpenAI moderation flags the query."""


class RetrievalError(RAGServiceError):
    """Raised when we cannot retrieve context from Chroma."""


@dataclass
class RetrievalContext:
    context: str
    pdf_links: List[str]
    embed_ids: List[str]


@dataclass
class RAGResult:
    answer: str
    pdf_links: List[str]
    prompt_type: Literal["greet", "answer", "fallback"]
    embed_ids: Optional[List[str]] = None


class RAGService:
    GREET_PATTERN = re.compile(r"^\s*(ì•ˆë…•|ã…ã…‡|í•˜ì´|hi|hello|í…ŒìŠ¤íŠ¸|ê³ ë§ˆì›Œ|ê°ì‚¬)\s*$", re.I)

    def __init__(
        self,
        *,
        db_path: str = "./chroma_db_v1",
        collection_name: str = "monthfarmtech_v1",
        n_results: int = 15,
        distance_threshold: float = 1.12,
        min_docs: int = 1,
        pdf_limit: int = 3,
        context_limit: int = 1800,
        openai_model: str = "gpt-4.1-mini",
        embedding_model: str = "text-embedding-3-small",
    ) -> None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RAGServiceError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self._client = OpenAI(api_key=api_key)
        self._chroma = PersistentClient(path=db_path)
        try:
            self._collection = self._chroma.get_collection(collection_name)
        except (NotFoundError, InvalidCollectionException) as exc:
            raise RetrievalError(
                f"'{collection_name}' ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Chroma DBë¥¼ ì ê²€í•´ì£¼ì„¸ìš”."
            ) from exc

        self._n_results = n_results
        self._distance_threshold = distance_threshold
        self._min_docs = min_docs
        self._pdf_limit = pdf_limit
        self._context_limit = context_limit
        self._openai_model = openai_model
        self._embedding_model = embedding_model

    def ask(self, raw_query: str) -> RAGResult:
        query = (raw_query or "").strip()
        if not query:
            raise EmptyQueryError("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        if self._is_inappropriate(query):
            raise InappropriateQueryError("ë¶€ì ì ˆí•˜ê±°ë‚˜ ì•ˆì „í•˜ì§€ ì•Šì€ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        prompt_type: Literal["greet", "answer", "fallback"]
        pdf_links: List[str] = []
        embed_ids: Optional[List[str]] = None

        if self.GREET_PATTERN.match(query):
            prompt = self._build_prompt_greet(query)
            prompt_type = "greet"
        else:
            retrieval = self._build_retrieval_context(query)
            if retrieval:
                prompt = self._build_prompt_answer(query, retrieval.context, retrieval.embed_ids, retrieval.pdf_links)
                pdf_links = retrieval.pdf_links
                embed_ids = retrieval.embed_ids
                prompt_type = "answer"
            else:
                prompt = self._build_prompt_fallback(query)
                prompt_type = "fallback"

        answer = self._call_gpt(prompt)
        return RAGResult(answer=answer, pdf_links=pdf_links, prompt_type=prompt_type, embed_ids=embed_ids)

    def _is_inappropriate(self, query: str) -> bool:
        moderation = self._client.moderations.create(model="omni-moderation-latest", input=query)
        return bool(moderation.results[0].flagged)

    def _call_gpt(self, prompt: str) -> str:
        try:
            response = self._client.responses.create(
                model=self._openai_model,
                input=[{"role": "user", "content": prompt}],
            )
        except Exception as exc:
            raise RAGServiceError("GPT í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") from exc
        return response.output_text

    def _build_retrieval_context(self, query: str) -> Optional[RetrievalContext]:
        try:
            embedding = self._client.embeddings.create(model=self._embedding_model, input=[query]).data[0].embedding
        except Exception as exc:  # OpenAI errors
            raise RAGServiceError("ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from exc

        try:
            query_result = self._collection.query(
                query_embeddings=[embedding],
                n_results=self._n_results,
                include=["documents", "metadatas", "distances", "ids"],
            )
        except Exception as exc:  # Chroma errors
            raise RetrievalError("ì§€ì‹ì„ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") from exc

        docs = query_result.get("documents", [[]])[0]
        metas = query_result.get("metadatas", [[]])[0]
        dists = query_result.get("distances", [[]])[0]
        ids_hit = query_result.get("ids", [[]])[0]

        kept = [
            (doc, meta or {}, id_hit, dist)
            for doc, meta, id_hit, dist in zip(docs, metas, ids_hit, dists)
            if dist <= self._distance_threshold
        ]

        if len(kept) < self._min_docs:
            return None

        context = "\n\n".join(doc for doc, _, _, _ in kept)[: self._context_limit]
        pdf_links = self._extract_pdf_links(kept)
        embed_ids = [id_hit for _, _, id_hit, _ in kept][: self._min_docs]
        return RetrievalContext(context=context, pdf_links=pdf_links, embed_ids=embed_ids)

    def _extract_pdf_links(self, records: Sequence[tuple[str, dict, str, float]]) -> List[str]:
        pdfs: List[str] = []
        for _, meta, _, _ in records:
            link = (meta.get("pdf_path") or meta.get("atchmnflUrl") or "").strip()
            if link and link not in pdfs:
                pdfs.append(link)
            if len(pdfs) >= self._pdf_limit:
                break
        return pdfs

    @staticmethod
    def _build_prompt_greet(_: str) -> str:
        return (
            "ì•„ì£¼ ì§§ê²Œ ì¸ì‚¬í•˜ê³ , ì´ ë´‡ì€ ë†ì—…(ì‘ë¬¼Â·ì¬ë°°Â·ë³‘í•´ì¶©) íŠ¹í™” ì±—ë´‡ì„ì„ ì•ˆë‚´í•œ ë’¤ "
            "ë†ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ ë‹¬ë¼ê³  ì •ì¤‘íˆ ìš”ì²­í•´ì¤˜. í•œë‘ ë¬¸ì¥ë§Œ."
        )

    @staticmethod
    def _build_prompt_answer(query: str, context: str, embed_ids: Sequence[str], pdf_links: Sequence[str]) -> str:
        links = "\n".join(pdf_links) if pdf_links else ""
        ids_str = ", ".join(embed_ids) if embed_ids else ""
        return (
            f"ì§ˆë¬¸: {query}\n\n"
            f"ê²€ìƒ‰ëœ ë‚´ìš©:\n{context}\n\n"
            f"ì°¸ê³  PDF ë§í¬:\n{links}\n\n"
            f"ê´€ë ¨ ì„ë² ë”© ID: {ids_str}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 10ì¤„ ì •ë„ë¡œ ë‹µë³€í•´ì£¼ê³  ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ë¼ê³  í•´ì¤˜."
        )

    @staticmethod
    def _build_prompt_fallback(query: str) -> str:
        return (
            f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
            "ì£¼ì œê°€ ë†ì—… ê´€ë ¨ì´ì§€ë§Œ í˜„ì¬ ì œê³µ ë°ì´í„°ì—ëŠ” ì¶©ë¶„í•œ ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤. "
            "ê°„ë‹¨í•œ ì¼ë°˜ ì •ë³´ ìˆ˜ì¤€ìœ¼ë¡œ 2~3ë¬¸ì¥ ìš”ì•½ ì œê³µ í›„, "
            "ë§ˆì§€ë§‰ì— 'ì´ ì£¼ì œëŠ” í˜„ì¬ ì €í¬ ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì•ˆë‚´ë“œë ¸ì–´ìš”."
            "ì‘ë¬¼ëª…Â·ì¦ìƒÂ·ì§€ì—­ì„ í•¨ê»˜ ì•Œë ¤ì£¼ì‹œë©´ ë” ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ™‚'ë¼ê³  ì•ˆë‚´í•´ì¤˜."
        )
