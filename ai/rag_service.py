"""
Utilities for running a RAG (Retrieval-Augmented Generation) workflow.

This module exposes a RAGService class that encapsulates:
* moderation checks
* greeting / fallback prompt construction
* embedding + retrieval against ChromaDB
* GPT response generation

It is designed to be imported by the FastAPI layer, so no CLI interaction
remains in this file.
"""

from __future__ import annotations

import os
import re
from dataclasses import dataclass
from typing import List, Literal, Optional, Sequence

from chromadb import PersistentClient
try:
    from chromadb.errors import InvalidCollectionException, NotFoundError
except ImportError:
    # chromadb ìµœì‹  ë²„ì „ í˜¸í™˜
    try:
        from chromadb.errors import InvalidCollectionException
    except ImportError:
        # InvalidCollectionExceptionë„ ì—†ëŠ” ê²½ìš°
        class InvalidCollectionException(Exception):  # type: ignore[misc]
            """Fallback for chromadb API."""

    class NotFoundError(InvalidCollectionException):  # type: ignore[misc]
        """Fallback for older chromadb API expectation."""
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


class RAGServiceError(RuntimeError):
    """Base class for RAG related errors."""


class EmptyQueryError(RAGServiceError):
    """Raised when the user query is empty."""


class InappropriateQueryError(RAGServiceError):
    """Raised when OpenAI moderation flags the query."""


class RetrievalError(RAGServiceError):
    """Raised when we cannot retrieve context from Chroma."""


@dataclass
class RetrievalContext:
    context: str
    pdf_links: List["ReferenceLink"]
    embed_ids: List[str]


@dataclass
class RAGResult:
    answer: str
    pdf_links: List["ReferenceLink"]
    prompt_type: Literal["greet", "answer", "fallback"]
    embed_ids: Optional[List[str]] = None


@dataclass
class ReferenceLink:
    title: str
    url: str


class RAGService:
    GREET_PATTERN = re.compile(r"^\s*(ì•ˆë…•|ã…ã…‡|í•˜ì´|hi|hello|í…ŒìŠ¤íŠ¸|ê³ ë§ˆì›Œ|ê°ì‚¬)\s*$", re.I)

    #  ëª¨ë¸/ê²½ë¡œ/ì„ê³„ê°’ ë“± ê³µí†µ ì„¤ì •ì„ ë¬¶ì–´ì„œ ì´í›„ í˜¸ì¶œì„ ë‹¨ìˆœí™”
    def __init__(
        self,
        *,
        db_path: str = "./chroma_db_v1",
        collection_name: str = "monthfarmtech_v1",
        n_results: int = 15,
        distance_threshold: float = 1.12,
        min_docs: int = 1,
        pdf_limit: int = 3,
        context_limit: int = 1800,
        openai_model: str = "gpt-4.1-mini",
        embedding_model: str = "text-embedding-3-small",
    ) -> None:
        # .envë¥¼ ë¡œë“œí•˜ê³  í‚¤ ì—†ìœ¼ë©´ ì˜ˆì™¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key: 
            raise RAGServiceError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self._client = OpenAI(api_key=api_key)
        # ChromaDB PersistentClientë¡œ ì»¬ë ‰ì…˜ open
        self._chroma = PersistentClient(path=db_path)
        try:  
            self._collection = self._chroma.get_collection(collection_name)
        except (NotFoundError, InvalidCollectionException) as exc:
            raise RetrievalError(
                f"'{collection_name}' ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Chroma DBë¥¼ ì ê²€í•´ì£¼ì„¸ìš”."
            ) from exc

        self._n_results = n_results
        self._distance_threshold = distance_threshold
        self._min_docs = min_docs
        self._pdf_limit = pdf_limit
        self._context_limit = context_limit
        self._openai_model = openai_model
        self._embedding_model = embedding_model

    # â€œìœ íš¨ì„± ê²€ì‚¬ â†’ ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ LLM í˜¸ì¶œ â†’ ê²°ê³¼ í¬ë§·â€ì„ ì›ìƒ·ìœ¼ë¡œ ì œê³µ.
    def ask(self, raw_query: str) -> RAGResult:
        query = (raw_query or "").strip()
        if not query:
            raise EmptyQueryError("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        if self._is_inappropriate(query):
            raise InappropriateQueryError("ë¶€ì ì ˆí•˜ê±°ë‚˜ ì•ˆì „í•˜ì§€ ì•Šì€ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        prompt_type: Literal["greet", "answer", "fallback"]
        pdf_links: List[ReferenceLink] = []
        embed_ids: Optional[List[str]] = None

        if self.GREET_PATTERN.match(query):
            prompt = self._build_prompt_greet(query)
            prompt_type = "greet"
        else:
            retrieval = self._build_retrieval_context(query)
            if retrieval:
                prompt = self._build_prompt_answer(query, retrieval.context, retrieval.embed_ids, retrieval.pdf_links)
                pdf_links = retrieval.pdf_links
                embed_ids = retrieval.embed_ids
                prompt_type = "answer"
            else:
                prompt = self._build_prompt_fallback(query)
                prompt_type = "fallback"

        answer = self._call_gpt(prompt)
        return RAGResult(answer=answer, pdf_links=pdf_links, prompt_type=prompt_type, embed_ids=embed_ids)

    # Moderation(ë¶€ì ì ˆ ì»¨í…ì¸ ) ì°¨ë‹¨
    def _is_inappropriate(self, query: str) -> bool:
        moderation = self._client.moderations.create(model="omni-moderation-latest", input=query)
        return bool(moderation.results[0].flagged)

    def _call_gpt(self, prompt: str) -> str:
        try:
            response = self._client.responses.create(
                model=self._openai_model,
                input=[{"role": "user", "content": prompt}],
            )
        except Exception as exc:
            raise RAGServiceError("GPT í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") from exc
        return response.output_text

    def _extract_crop_name(self, query: str, metas, kept_idx):
        """
        ì§ˆë¬¸ì—ì„œ í•œê¸€ ëª…ì‚¬ í›„ë³´ë¥¼ ë½‘ê³ ,
        RAG ë©”íƒ€ë°ì´í„°(title/curationNm)ì— ì‹¤ì œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë©´ ì‘ë¬¼ëª…ìœ¼ë¡œ ê°„ì£¼.
        """
        tokens = re.findall(r"[ê°€-í£]+", query)
        cands = [t for t in tokens if len(t) >= 2]
        if not cands:
            return None

        titles = []
        for i in kept_idx:
            m = metas[i]
            title = (m.get("title") or m.get("curationNm") or "").strip()
            if title:
                titles.append(title)

        if not titles:
            return None

        joined = " ".join(titles)
        for cand in cands:
            if cand in joined:
                return cand

        return None


    def _build_retrieval_context(self, query: str) -> Optional[RetrievalContext]:
        try:
            embedding = self._client.embeddings.create(model=self._embedding_model, input=[query]).data[0].embedding
        except Exception as exc:  # OpenAI errors
            raise RAGServiceError("ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from exc

        try:
            query_result = self._collection.query(
                query_embeddings=[embedding],
                n_results=self._n_results,
                include=["documents", "metadatas", "distances"],
            )
        except Exception as exc:  # Chroma errors
            raise RetrievalError("ì§€ì‹ì„ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") from exc

        docs = query_result.get("documents", [[]])[0]
        metas = query_result.get("metadatas", [[]])[0]
        dists = query_result.get("distances", [[]])[0]
        ids_hit = query_result.get("ids", [[]])[0]

        kept = [
            (doc, meta or {}, id_hit, dist)
            for doc, meta, id_hit, dist in zip(docs, metas, ids_hit, dists)
            if dist <= self._distance_threshold
        ]

        # í•„í„° ê²°ê³¼ê°€ ìˆìœ¼ë©´ êµì²´, ì—†ìœ¼ë©´ ê¸°ì¡´ kept ì‚¬ìš©
        if len(kept) < self._min_docs:
            return None

        # ì‘ë¬¼ í•„í„°ë§ ì¶”ê°€
        kept_idx = list(range(len(kept)))
        crop = self._extract_crop_name(query, metas, kept_idx)

        if crop:
            filtered = []
            for idx in kept_idx:
                doc, meta, id_hit, dist = kept[idx]
                title = (meta.get("title") or meta.get("curationNm") or "")
                if crop in title or crop in doc:
                    filtered.append(idx)
            if filtered:
                kept = [kept[i] for i in filtered]

        context = "\n\n".join(doc for doc, _, _, _ in kept)[: self._context_limit]
        pdf_links = self._extract_pdf_links(kept)
        embed_ids = [id_hit for _, _, id_hit, _ in kept][: self._min_docs]
        return RetrievalContext(context=context, pdf_links=pdf_links, embed_ids=embed_ids)

    def _extract_pdf_links(self, records: Sequence[tuple[str, dict, str, float]]) -> List[ReferenceLink]:
        pdfs: List[ReferenceLink] = []
        seen_urls: set[str] = set()
        for _, meta, _, _ in records:
            raw_url = (meta.get("pdf_path") or meta.get("atchmnflUrl") or meta.get("linkUrl") or "").strip()
            if not raw_url or raw_url in seen_urls:
                continue
            title = (meta.get("title") or meta.get("curationNm") or meta.get("document_title") or "").strip()
            if not title:
                title = raw_url
            pdfs.append(ReferenceLink(title=title, url=raw_url))
            seen_urls.add(raw_url)
            if len(pdfs) >= self._pdf_limit:
                break
        return pdfs

    @staticmethod
    def _build_prompt_greet(_: str) -> str:
        return (
            "ì•„ì£¼ ì§§ê²Œ ì¸ì‚¬í•˜ê³ , ì´ ë´‡ì€ ë†ì—…(ì‘ë¬¼Â·ì¬ë°°Â·ë³‘í•´ì¶©) íŠ¹í™” ì±—ë´‡ì„ì„ ì•ˆë‚´í•œ ë’¤ "
            "ë†ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ ë‹¬ë¼ê³  ì •ì¤‘íˆ ìš”ì²­í•´ì¤˜. í•œë‘ ë¬¸ì¥ë§Œ."
        )

    @staticmethod
    def _build_prompt_answer(query: str, context: str, embed_ids: Sequence[str], pdf_links: Sequence[ReferenceLink]) -> str:
        links = "\n".join(f"{link.title}: {link.url}" for link in pdf_links) if pdf_links else ""
        ids_str = ", ".join(embed_ids) if embed_ids else ""
        return (
            f"ì§ˆë¬¸: {query}\n\n"
            f"ê²€ìƒ‰ëœ ë‚´ìš©:\n{context}\n\n"
            f"ì°¸ê³  PDF ë§í¬:\n{links}\n\n"
            f"ê´€ë ¨ ì„ë² ë”© ID: {ids_str}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 10ì¤„ ì •ë„ë¡œ ë‹µë³€í•´ì£¼ê³  ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ë¼ê³  í•´ì¤˜."
        )

    @staticmethod
    def _build_prompt_fallback(query: str) -> str:
        return (
            f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
            "ì£¼ì œê°€ ë†ì—… ê´€ë ¨ì´ì§€ë§Œ í˜„ì¬ ì œê³µ ë°ì´í„°ì—ëŠ” ì¶©ë¶„í•œ ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤. "
            "ê°„ë‹¨í•œ ì¼ë°˜ ì •ë³´ ìˆ˜ì¤€ìœ¼ë¡œ 2~3ë¬¸ì¥ ìš”ì•½ ì œê³µ í›„, "
            "ë§ˆì§€ë§‰ì— 'ì´ ì£¼ì œëŠ” í˜„ì¬ ì €í¬ ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì•ˆë‚´ë“œë ¸ì–´ìš”."
            "ì‘ë¬¼ëª…Â·ì¦ìƒÂ·ì§€ì—­ì„ í•¨ê»˜ ì•Œë ¤ì£¼ì‹œë©´ ë” ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ™‚'ë¼ê³  ì•ˆë‚´í•´ì¤˜."
        )
